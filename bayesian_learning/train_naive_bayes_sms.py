"""
train_naive_bayes_sms.py

Script ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Multinomial Naive Bayes Î³Î¹Î± Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·
SMS Î¼Î·Î½Ï…Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ "ham" (ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¬) ÎºÎ±Î¹ "spam" (Î±Î½ÎµÏ€Î¹Î¸ÏÎ¼Î·Ï„Î±).

Î¤Î¿ script ÎµÎ¯Î½Î±Î¹ Î³ÏÎ±Î¼Î¼Î­Î½Î¿ Î¼Îµ Ï€Î¿Î»Î»Î¬ ÏƒÏ‡ÏŒÎ»Î¹Î± ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ ÏÏƒÏ„Îµ Î½Î± ÎµÎ¯Î½Î±Î¹
ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿ Î³Î¹Î± Ï†Î¿Î¹Ï„Î·Ï„Î­Ï‚ Ï€Î¿Ï… Î¾ÎµÎºÎ¹Î½Î¿ÏÎ½ Ï„ÏÏÎ± Î¼Îµ Machine Learning.

Î— Î²Î±ÏƒÎ¹ÎºÎ® Î¹Î´Î­Î± Ï„Î¿Ï… Naive Bayes:

- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿Î½ Ï„ÏÏ€Î¿ Ï„Î¿Ï… Bayes:
  P(y | x) âˆ P(x | y) P(y)
- Î¥Ï€Î¿Î¸Î­Ï„Î¿Ï…Î¼Îµ ÏŒÏ„Î¹ Ï„Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ x_i ÎµÎ¯Î½Î±Î¹ Î±Î½ÎµÎ¾Î¬ÏÏ„Î·Ï„Î± Î¼ÎµÏ„Î±Î¾Ï Ï„Î¿Ï…Ï‚
  Î´ÎµÎ´Î¿Î¼Î­Î½Î·Ï‚ Ï„Î·Ï‚ ÎºÎ»Î¬ÏƒÎ·Ï‚ y (Ï…Ï€ÏŒÎ¸ÎµÏƒÎ· "naive").
- Î“Î¹Î± ÎºÎµÎ¯Î¼ÎµÎ½Î¿, Ï„Î¿ Multinomial Naive Bayes Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¹Ï‚ ÏƒÏ…Ï‡Î½ÏŒÏ„Î·Ï„ÎµÏ‚
  ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·Ï‚ Ï„Ï‰Î½ Î»Î­Î¾ÎµÏ‰Î½ Î¼Î­ÏƒÎ± ÏƒÎµ ÎºÎ¬Î¸Îµ ÎºÎ»Î¬ÏƒÎ·.

Î•Î´Ï, Ï„Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï€ÏÎ¿ÎºÏÏ€Ï„Î¿Ï…Î½ Î±Ï€ÏŒ TfidfVectorizer Ï€Î¬Î½Ï‰ ÏƒÏ„Î± ÎºÎµÎ¯Î¼ÎµÎ½Î±
Ï„Ï‰Î½ SMS, ÎºÎ±Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ MultinomialNB (Ï„Î·Ï‚ scikit-learn).
"""

from pathlib import Path

import joblib
import matplotlib
matplotlib.use("Agg")  # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ backend Ï‡Ï‰ÏÎ¯Ï‚ Î¿Î¸ÏŒÎ½Î· Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÎµÎ¹ÎºÏŒÎ½Ï‰Î½
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline


# -------------------------------------------------------------
# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î²Î±ÏƒÎ¹ÎºÏÎ½ paths
# -------------------------------------------------------------

# Î¡Î¹Î¶Î¹ÎºÏŒÏ‚ Ï†Î¬ÎºÎµÎ»Î¿Ï‚ Ï„Î¿Ï… repo (Î¸ÎµÏ‰ÏÎ¿ÏÎ¼Îµ ÏŒÏ„Î¹ Î¿ Ï†Î¬ÎºÎµÎ»Î¿Ï‚ bayesian_learning
# Î²ÏÎ¯ÏƒÎºÎµÏ„Î±Î¹ Î±Ï€ÎµÏ…Î¸ÎµÎ¯Î±Ï‚ ÎºÎ¬Ï„Ï‰ Î±Ï€ÏŒ Ï„Î· ÏÎ¯Î¶Î±).
ROOT = Path(__file__).resolve().parents[1]

# Î‘ÏÏ‡ÎµÎ¯Î¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (CSV) Ï€Î¿Ï… Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ data/
DATA_PATH = ROOT / "data" / "sms_spam.csv"

# Î¦Î¬ÎºÎµÎ»Î¿Ï‚ Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÎºÎ±Î¹ Î´Î¹Î±Î³ÏÎ±Î¼Î¼Î¬Ï„Ï‰Î½
MODELS_DIR = Path(__file__).resolve().parent / "models"
MODELS_DIR.mkdir(exist_ok=True)


def load_sms_spam():
    """
    Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î¿ SMS Spam dataset Î±Ï€ÏŒ Ï„Î¿ data/sms_spam.csv.

    Î‘Î½Î±Î¼Î­Î½ÎµÏ„Î±Î¹ CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚:
    - 'label': Ï„Î¹Î¼Î® 'ham' Î® 'spam'
    - 'text' : Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Ï„Î¿Ï… SMS (string)

    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹:
    - X: pandas Series Î¼Îµ Ï„Î± ÎºÎµÎ¯Î¼ÎµÎ½Î±
    - y: pandas Series Î¼Îµ Ï„Î¹Ï‚ ÎµÏ„Î¹ÎºÎ­Ï„ÎµÏ‚ ÏƒÎµ Î¼Î¿ÏÏ†Î® 0/1
      (0 = ham, 1 = spam)
    """
    if not DATA_PATH.exists():
        raise FileNotFoundError(
            f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½: {DATA_PATH}\n"
            "Î’ÎµÎ²Î±Î¹ÏÏƒÎ¿Ï… ÏŒÏ„Î¹ Î­Ï‡ÎµÎ¹Ï‚ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏƒÎµÎ¹ Ï„Î¿ SMS Spam dataset Ï‰Ï‚ "
            "'sms_spam.csv' ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ data/."
        )

    # Î¦Î¿ÏÏ„ÏÎ½Î¿Ï…Î¼Îµ Î¼Îµ encoding='latin-1' Î³Î¹Î±Ï„Î¯ Ï„Î¿ sms_spam.csv Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ UTF-8
    df = pd.read_csv(DATA_PATH, encoding='latin-1')

    # Î¤Î¿ Î±ÏÏ‡Î¹ÎºÏŒ CSV Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»ÎµÏ‚ v1 (label) ÎºÎ±Î¹ v2 (text)
    # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ Î±Ï…Ï„Î­Ï‚ Ï„Î¹Ï‚ Î´ÏÎ¿ ÎºÎ±Î¹ Î¼ÎµÏ„Î¿Î½Î¿Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î³Î¹Î± ÎµÏ…ÎºÎ¿Î»Î¯Î±
    df = df[['v1', 'v2']].copy()
    df.columns = ['label', 'text']

    # Î‘Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ Ï„Ï…Ï‡ÏŒÎ½ Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Îµ ÎºÎµÎ½Î¬ ÏƒÏ„Î¹Ï‚ Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚
    df = df.dropna(subset=['label', 'text']).copy()

    # ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· labels (strip ÎºÎ±Î¹ lowercase)
    df['label'] = df['label'].str.strip().str.lower()
    # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Îµ 'ham' Î® 'spam'
    df = df[df['label'].isin(['ham', 'spam'])].copy()

    # Î§Î±ÏÏ„Î¿Î³ÏÎ¬Ï†Î·ÏƒÎ· Ï„Ï‰Î½ labels ÏƒÎµ 0/1
    # ham -> 0 (ÎºÎ±Î½Î¿Î½Î¹ÎºÏŒ Î¼Î®Î½Ï…Î¼Î±)
    # spam -> 1 (Î±Î½ÎµÏ€Î¹Î¸ÏÎ¼Î·Ï„Î¿)
    y = (df["label"] == "spam").astype(int)

    # Î¤Î± ÎºÎµÎ¯Î¼ÎµÎ½Î± Ï„Ï‰Î½ Î¼Î·Î½Ï…Î¼Î¬Ï„Ï‰Î½
    X = df["text"].astype(str)

    return X, y


def train(alpha: float = 1.0, max_features: int = 10000, test_size: float = 0.2, random_state: int = 0):
    """
    Î•ÎºÏ€Î±Î¹Î´ÎµÏÎµÎ¹ Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î¿ Multinomial Naive Bayes Î³Î¹Î± Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· SMS.

    Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹
    ----------
    alpha : float
        Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Ï‚ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚ Laplace (MultinomialNB.alpha).
        Î¤Î¹Î¼Î­Ï‚ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎµÏ‚ Ï„Î¿Ï… 0 Î±Ï€Î¿Ï„ÏÎ­Ï€Î¿Ï…Î½ Î¼Î·Î´ÎµÎ½Î¹ÎºÎ­Ï‚ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚.

    max_features : int
        ÎœÎ­Î³Î¹ÏƒÏ„Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ TF-IDF Ï€Î¿Ï… Î¸Î± ÎºÏÎ±Ï„Î®ÏƒÎ¿Ï…Î¼Îµ.
        Î‘Î½ Ï„Î¿ dataset Î­Ï‡ÎµÎ¹ Ï€Î¬ÏÎ± Ï€Î¿Î»Î»Î­Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚,
        Ï€ÎµÏÎ¹Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Î»ÎµÎ¾Î¹Î»ÏŒÎ³Î¹Î¿ ÏƒÎµ max_features ÏŒÏÎ¿Ï…Ï‚.

    test_size : float
        Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸Î¿ÏÎ½ Î³Î¹Î± validation
        (Ï€.Ï‡. 0.2 = 20% Ï„Î¿Ï… dataset).

    random_state : int
        Î£Ï€ÏŒÏÎ¿Ï‚ Ï„Ï…Ï‡Î±Î¹ÏŒÏ„Î·Ï„Î±Ï‚ Î³Î¹Î± Î±Î½Î±Ï€Î±ÏÎ±Î³Ï‰Î³Î¹Î¼ÏŒÏ„Î·Ï„Î± Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½.
    """
    # ---------------------------------------------------------
    # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    # ---------------------------------------------------------
    X, y = load_sms_spam()

    # Î§Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÎµ train / validation Î¼Îµ stratify ÏÏƒÏ„Îµ Î½Î± Î´Î¹Î±Ï„Î·ÏÎ®ÏƒÎ¿Ï…Î¼Îµ
    # Ï„Î·Î½ Î±Î½Î±Î»Î¿Î³Î¯Î± ham / spam ÎºÎ±Î¹ ÏƒÏ„Î± Î´ÏÎ¿ ÏƒÏÎ½Î¿Î»Î±.
    X_train, X_val, y_train, y_val = train_test_split(
        X,
        y,
        test_size=test_size,
        stratify=y,
        random_state=random_state,
    )

    # ---------------------------------------------------------
    # 2. ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Pipeline (TF-IDF Vectorizer + MultinomialNB)
    # ---------------------------------------------------------
    # Î¤Î¿ Pipeline ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ Î´Î¹Î±Î´Î¿Ï‡Î¹ÎºÎ¬ Î¼ÎµÏ„Î±ÏƒÏ‡Î·Î¼Î±Ï„Î¹ÏƒÏ„Î­Ï‚ ÎºÎ±Î¹ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Ï„Î®.
    # Î•Î´Ï:
    # - Ï€ÏÏÏ„Î± Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Ï„Î± ÎºÎµÎ¯Î¼ÎµÎ½Î± ÏƒÎµ TF-IDF features,
    # - Î¼ÎµÏ„Î¬ ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ Ï„Î¿Î½ Multinomial Naive Bayes.
    pipe = Pipeline(
        steps=[
            (
                "tfidf",
                TfidfVectorizer(
                    max_features=max_features,
                    stop_words="english",  # Ï„Î¿ dataset ÎµÎ¯Î½Î±Î¹ ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ ÏƒÏ„Î± Î±Î³Î³Î»Î¹ÎºÎ¬
                ),
            ),
            ("nb", MultinomialNB(alpha=alpha)),
        ]
    )

    # ---------------------------------------------------------
    # 3. Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
    # ---------------------------------------------------------
    pipe.fit(X_train, y_train)

    # ---------------------------------------------------------
    # 4. Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÏ„Î¿ validation set
    # ---------------------------------------------------------
    y_pred = pipe.predict(X_val)

    print("=== Multinomial Naive Bayes ÏƒÏ„Î¿ SMS Spam dataset ===")
    print(f"Î”ÎµÎ¯Î³Î¼Î±Ï„Î± train: {len(X_train)}, validation: {len(X_val)}")
    print()
    print(
        classification_report(
            y_val,
            y_pred,
            target_names=["ham", "spam"],
            digits=3,
        )
    )

    # ---------------------------------------------------------
    # 5. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î¿Ï… pipeline (vectorizer + Î¼Î¿Î½Ï„Î­Î»Î¿)
    # ---------------------------------------------------------
    model_file = MODELS_DIR / f"naive_bayes_sms_alpha{alpha}.joblib"
    joblib.dump(pipe, model_file)
    print(f"\nâœ… Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÏƒÏ„Î¿: {model_file}")

    # ---------------------------------------------------------
    # 6. Î•ÏÎ¼Î·Î½ÎµÎ¯Î±: priors ÎºÎ±Î¹ Ï€Î¹Î¿ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚
    # ---------------------------------------------------------
    nb = pipe.named_steps["nb"]
    tfidf = pipe.named_steps["tfidf"]

    print("\nLog-priors ÎºÎ»Î¬ÏƒÎµÏ‰Î½ (P(class)) ÏŒÏ€Ï‰Ï‚ Ï„Î± Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿:")
    for cls, logp in zip(["ham", "spam"], nb.class_log_prior_):
        print(f"  {cls:>4}: {logp: .3f}")

    # ÎŸÎ¹ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚ Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎºÎ»Î¬ÏƒÎ·
    feature_names = np.array(tfidf.get_feature_names_out())

    # Î”ÎµÎ¯Ï‡Î½Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Ï€Î¹Î¿ "Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ­Ï‚" Î»Î­Î¾ÎµÎ¹Ï‚ Î³Î¹Î± spam ÎºÎ±Î¹ ham
    spam_top_idx = nb.feature_log_prob_[1].argsort()[-15:][::-1]
    ham_top_idx = nb.feature_log_prob_[0].argsort()[-15:][::-1]

    print("\nTop-15 Î»Î­Î¾ÎµÎ¹Ï‚ Î¼Îµ Ï„Î· Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î± ÏƒÏ„Î·Î½ ÎºÎ»Î¬ÏƒÎ· 'spam':")
    print(", ".join(feature_names[spam_top_idx]))

    print("\nTop-15 Î»Î­Î¾ÎµÎ¹Ï‚ Î¼Îµ Ï„Î· Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î± ÏƒÏ„Î·Î½ ÎºÎ»Î¬ÏƒÎ· 'ham':")
    print(", ".join(feature_names[ham_top_idx]))

    # ---------------------------------------------------------
    # 7. Confusion matrix ÏƒÏ„Î¿ validation set
    # ---------------------------------------------------------
    cm = confusion_matrix(y_val, y_pred)

    fig, ax = plt.subplots(figsize=(4, 4))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["ham", "spam"])
    disp.plot(ax=ax)
    ax.set_title("Confusion matrix (validation set)")
    fig.tight_layout()

    fig_path = MODELS_DIR / f"naive_bayes_sms_alpha{alpha}_cm.png"
    fig.savefig(fig_path, dpi=150)
    plt.close(fig)
    print(f"ğŸ“Š Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ Ï„Î¿ Î´Î¹Î¬Î³ÏÎ±Î¼Î¼Î± confusion matrix ÏƒÏ„Î¿: {fig_path}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Multinomial Naive Bayes ÏƒÏ„Î¿ SMS Spam dataset."
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=1.0,
        help="Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Ï‚ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚ Laplace (MultinomialNB.alpha).",

    )
    parser.add_argument(
        "--max_features",
        type=int,
        default=10000,
        help="ÎœÎ­Î³Î¹ÏƒÏ„Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ TF-IDF.",
    )
    parser.add_argument(
        "--test_size",
        type=float,
        default=0.2,
        help="ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ validation set Ï‰Ï‚ Ï€Î¿ÏƒÎ¿ÏƒÏ„ÏŒ (0â€“1).",

    )
    parser.add_argument(
        "--random_state",
        type=int,
        default=0,
        help="Î£Ï€ÏŒÏÎ¿Ï‚ Ï„Ï…Ï‡Î±Î¹ÏŒÏ„Î·Ï„Î±Ï‚ Î³Î¹Î± Î±Î½Î±Ï€Î±ÏÎ±Î³Ï‰Î³Î¹Î¼ÏŒÏ„Î·Ï„Î±.",
    )

    args = parser.parse_args()

    train(
        alpha=args.alpha,
        max_features=args.max_features,
        test_size=args.test_size,
        random_state=args.random_state,
    )
