{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Gaussian Naive Bayes στο Iris dataset\n",
    "\n",
    "Σε αυτό το notebook θα δούμε ένα απλό παράδειγμα χρήσης\n",
    "**Gaussian Naive Bayes** στο κλασικό **Iris dataset**.\n",
    "\n",
    "## Στόχοι\n",
    "\n",
    "- Να θυμηθούμε τη βασική ιδέα του Naive Bayes.\n",
    "- Να δούμε πότε χρησιμοποιούμε την παραλλαγή **Gaussian**.\n",
    "- Να εκπαιδεύσουμε GaussianNB στο Iris dataset.\n",
    "- Να αξιολογήσουμε την απόδοση με confusion matrix.\n",
    "- Να δούμε posterior πιθανότητες για μερικά δείγματα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Θεωρία: Gaussian Naive Bayes\n",
    "\n",
    "Σε αυτό το τμήμα περιγράφουμε τη θεωρία πίσω από τον ταξινομητή **Gaussian Naive Bayes** (Γκαουσιανό Naive Bayes classifier), ξεκινώντας από τον γενικό κανόνα του Naive Bayes, περνώντας στο Γκαουσιανό μοντέλο με συναρτήσεις πυκνότητας πιθανότητας (probability density functions, **pdf**) και φτάνοντας στη λογαριθμική κλίμακα (log-scale), τον υπολογισμό των εκ των υστέρων πιθανοτήτων (posterior probabilities) και τη χρήση τυποποίησης (standardization) με z-scores.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Γενικός κανόνας Naive Bayes\n",
    "\n",
    "Θεωρούμε ένα διάνυσμα χαρακτηριστικών (feature vector) \n",
    "$$\n",
    "x = (x_1, x_2, \\ldots, x_d)\n",
    "$$\n",
    "και μια διακριτή μεταβλητή κλάσης (class label) $y$ που παίρνει τιμές σε ένα σύνολο κλάσεων $\\{1, \\dots, K\\}$.\n",
    "\n",
    "Ο βασικός κανόνας του **Naive Bayes** βασίζεται στον κανόνα του Bayes και στην υπόθεση συνθήκης ανεξαρτησίας (conditional independence assumption) των χαρακτηριστικών, δεδομένης της κλάσης, και μας δίνει την **εκ των υστέρων πιθανότητα** (posterior probability) $P(y \\mid x)$ ως:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) \\propto P(y) P(x \\mid y)\n",
    "= P(y) \\prod_{i=1}^d P(x_i \\mid y).\n",
    "$$\n",
    "\n",
    "(πλήρης κανονικοποιημένος τύπος Bayes:)\n",
    "\n",
    "$$\n",
    "P(y \\mid x)\n",
    "= \\frac{P(y)\\,P(x \\mid y)}{\\sum_{k} P(y=k)\\,P(x \\mid y=k)}.\n",
    "$$\n",
    "\n",
    "- $P(y)$: εκ των προτέρων πιθανότητα (prior probability) για την κλάση $y$.\n",
    "- $P(x \\mid y)$: πιθανοφάνεια (likelihood) των δεδομένων $x$ δεδομένης της κλάσης $y$.\n",
    "- $P(y \\mid x)$: εκ των υστέρων πιθανότητα (posterior probability) της κλάσης $y$ δεδομένων των χαρακτηριστικών $x$.\n",
    "- $P(x_i \\mid y)$: επιμέρους όροι πιθανοφάνειας (likelihood terms) για κάθε χαρακτηριστικό (feature).\n",
    "\n",
    "Η υπόθεση Naive (Naive assumption) είναι ότι, δεδομένης της κλάσης $y$, τα χαρακτηριστικά $x_i$ είναι ανεξάρτητα μεταξύ τους· γι’ αυτό η πιθανοφάνεια γράφεται ως γινόμενο.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Γκαουσιανό Naive Bayes και συνάρτηση πυκνότητας πιθανότητας (probability density function, pdf)\n",
    "\n",
    "Στο **Gaussian Naive Bayes** υποθέτουμε ότι κάθε χαρακτηριστικό (feature) $x_i$, για κάθε κλάση $y = k$, ακολουθεί Κανονική κατανομή (Gaussian distribution). Δηλαδή:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y = k) \n",
    "= \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2)\n",
    "= \\frac{1}{\\sqrt{2\\pi\\sigma_{k,i}^2}}\n",
    "  \\exp\\left(-\\frac{(x_i-\\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right),\n",
    "$$\n",
    "\n",
    "όπου:\n",
    "\n",
    "- $\\mu_{k,i}$: μέση τιμή (mean) του χαρακτηριστικού $i$ στην κλάση $k$,\n",
    "- $\\sigma_{k,i}^2$: διακύμανση (variance) του χαρακτηριστικού $i$ στην κλάση $k$.\n",
    "\n",
    "Αυτή η συνάρτηση είναι η **συνάρτηση πυκνότητας πιθανότητας** (probability density function, **pdf**) της Κανονικής κατανομής.\n",
    "\n",
    "Για ένα πλήρες διάνυσμα χαρακτηριστικών $x$, η συνολική πιθανοφάνεια (likelihood) υπό την κλάση $y = k$ γράφεται ως γινόμενο των Γκαουσιανών:\n",
    "\n",
    "$$\n",
    "P(x \\mid y = k) = \\prod_{i=1}^d \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2).\n",
    "$$\n",
    "\n",
    "Επομένως η εκ των υστέρων πιθανότητα (posterior probability) γράφεται:\n",
    "\n",
    "$$\n",
    "P(y = k \\mid x) \\propto P(y = k)\n",
    "\\prod_{i=1}^d \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2).\n",
    "$$\n",
    "\n",
    "Το Gaussian Naive Bayes είναι ιδιαίτερα κατάλληλο όταν τα χαρακτηριστικά είναι\n",
    "**συνεχείς αριθμητικές μεταβλητές** (continuous numeric features), π.χ. μήκη, βάρη, φυσικές μετρήσεις κ.λπ.\n",
    "\n",
    "Αν για ένα συγκεκριμένο χαρακτηριστικό (π.χ. μήκος πετάλου (petal length)) οι μέσοι και οι διακυμάνσεις διαφέρουν έντονα ανά κλάση, τότε η αντίστοιχη Γκαουσιανή pdf συμβάλλει ισχυρή διακριτική πληροφορία στο $P(x \\mid y = k)$, αυξάνοντας τη διαχωρισιμότητα (separability) των κλάσεων.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Μάθηση παραμέτρων από δεδομένα (empirical parameter estimation)\n",
    "\n",
    "Κατά την εκπαίδευση του μοντέλου Gaussian Naive Bayes, οι παράμετροι εκτιμώνται **εμπειρικά** (empirically) από τα δεδομένα εκπαίδευσης (training data).\n",
    "\n",
    "Στη scikit-learn, με την κλήση:\n",
    "\n",
    "```python\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "το αντικείμενο `GaussianNB` εκτιμά:\n",
    "\n",
    "- $N_k$ = πλήθος δειγμάτων στην κλάση (class) $k$  \n",
    "  → `gnb.class_count_`.\n",
    "\n",
    "- $\\hat{\\pi}_k = \\dfrac{N_k}{N}$ = εμπειρική εκ των προτέρων πιθανότητα (empirical prior probability) για την κλάση $k$  \n",
    "  → `gnb.class_prior_`.\n",
    "\n",
    "- $\\hat{\\mu}_{k,i} = \\dfrac{1}{N_k} \\sum_{n: y_n=k} x_{n,i}$  \n",
    "  μέσος (mean) του χαρακτηριστικού $i$ στην κλάση $k$  \n",
    "  → `gnb.theta_`.\n",
    "\n",
    "- $\\hat{\\sigma}^2_{k,i} = \\dfrac{1}{N_k} \\sum_{n: y_n=k} (x_{n,i} - \\hat{\\mu}_{k,i})^2$  \n",
    "  διακύμανση (variance) του χαρακτηριστικού $i$ στην κλάση $k$  \n",
    "  → `gnb.var_`.\n",
    "\n",
    "Αυτές οι εκτιμήσεις είναι τυπικά **μέγιστης πιθανοφάνειας** (maximum likelihood estimates, MLE) υπό την υπόθεση της Γκαουσιανής κατανομής.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Συνάρτηση πυκνότητας πιθανότητας (pdf) σε λογαριθμική κλίμακα (log-scale)\n",
    "\n",
    "Στην πράξη, το Gaussian Naive Bayes δεν δουλεύει απευθείας με τις τιμές της pdf \n",
    "$P(x_i \\mid y=k)$, αλλά με τον **λογάριθμο της pdf** (log-pdf) για κάθε χαρακτηριστικό.  \n",
    "\n",
    "Για την Κανονική κατανομή έχουμε:\n",
    "\n",
    "$$\n",
    "\\log P(x_i \\mid y=k)\n",
    "= \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2)\n",
    "= -\\frac{1}{2}\n",
    "\\left[\n",
    "\\log(2\\pi\\sigma_{k,i}^2)\n",
    "+\n",
    "\\frac{(x_i - \\mu_{k,i})^2}{\\sigma_{k,i}^2}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Παρατηρούμε ότι:\n",
    "\n",
    "- Ο πολλαπλασιαστικός παράγοντας \n",
    "  $\\dfrac{1}{\\sqrt{2\\pi\\sigma_{k,i}^2}}$\n",
    "  γίνεται πρόσθετος όρος \n",
    "  $-\\frac{1}{2}\\log(2\\pi\\sigma_{k,i}^2)$\n",
    "  στο log-scale.\n",
    "- Ο εκθετικός όρος \n",
    "  $\\exp\\left(-\\frac{(x_i-\\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right)$\n",
    "  μετατρέπεται σε γραμμικό όρο \n",
    "  $-\\dfrac{(x_i-\\mu_{k,i})^2}{2\\sigma_{k,i}^2}$\n",
    "  μέσα στα brackets.\n",
    "\n",
    "Για ένα πλήρες διάνυσμα χαρακτηριστικών $x$, η log-πιθανοφάνεια (log-likelihood) για την κλάση $k$ είναι:\n",
    "\n",
    "$$\n",
    "\\log P(x \\mid y = k)\n",
    "= \\sum_{i=1}^d \\log P(x_i \\mid y = k).\n",
    "$$\n",
    "\n",
    "Αυτό αντιστοιχεί στον κώδικα μιας συνάρτησης τύπου `log_gaussian_pdf(x, mean, var)` που υπολογίζει τον log-pdf για κάθε χαρακτηριστικό και στη συνέχεια τα αθροίζει.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Λογαριθμική κλίμακα (log-scale), log-likelihood και αριθμητική σταθερότητα (numerical stability)\n",
    "\n",
    "Αντί να δουλεύουμε με τα γινόμενα πιθανοτήτων στο αρχικό τους μέγεθος, περνάμε σε **λογαριθμική κλίμακα (log-scale)**:\n",
    "\n",
    "- Αν\n",
    "  $$\n",
    "  P(x \\mid y=k) = \\prod_{i=1}^d p_{k,i},\n",
    "  $$\n",
    "  τότε\n",
    "  $$\n",
    "  \\log P(x \\mid y=k) = \\sum_{i=1}^d \\log p_{k,i}.\n",
    "  $$\n",
    "\n",
    "Αυτό έχει τρία βασικά πλεονεκτήματα:\n",
    "\n",
    "1. **Αριθμητική σταθερότητα (numerical stability)**  \n",
    "   Οι πιθανότητες είναι αριθμοί στο $(0,1)$ και όταν πολλαπλασιάζουμε πολλά τέτοια νούμερα, το γινόμενο μπορεί να γίνει τόσο μικρό που να «υποχειλίσει» (underflow) σε υπολογιστή και να μη διακρίνεται από το 0.  \n",
    "   Με τους λογαρίθμους:\n",
    "   - μετατρέπουμε το γινόμενο σε άθροισμα,\n",
    "   - δουλεύουμε με (συνήθως) μέτριες αρνητικές τιμές (π.χ. -5, -10, -100 αντί για $10^{-23}$),\n",
    "   - αποφεύγουμε αριθμητικά σφάλματα υποχειλισμού.\n",
    "\n",
    "2. **Απλούστεροι υπολογισμοί (simpler computations)**  \n",
    "   Στο Naive Bayes η πιθανοφάνεια είναι γινόμενο πιθανοτήτων. Σε log-κλίμακα:\n",
    "   - οι log-likelihoods απλά αθροίζονται,\n",
    "   - μπορούμε να προσθέσουμε εύκολα log-priors και log-likelihoods για να πάρουμε log-posteriors.\n",
    "\n",
    "3. **Αναλλοίωτο του argmax (argmax invariance)**  \n",
    "   Για την απόφαση κλάσης χρησιμοποιούμε τον εκτιμητή μέγιστης εκ των υστέρων πιθανότητας (maximum a posteriori estimator, **MAP**):\n",
    "\n",
    "   $$\n",
    "   \\hat y(x) = \\arg\\max_k P(y=k \\mid x).\n",
    "   $$\n",
    "\n",
    "   Επειδή ο λογάριθμος είναι **αυστηρά αύξουσα** συνάρτηση, ισχύει:\n",
    "\n",
    "   $$\n",
    "   \\arg\\max_k P(y=k \\mid x)\n",
    "   = \\arg\\max_k \\log P(y=k \\mid x).\n",
    "   $$\n",
    "\n",
    "   Άρα μπορούμε να δουλεύουμε αποκλειστικά με log-τιμές χωρίς να αλλάζει η τελική απόφαση του ταξινομητή.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Log-posterior στο Gaussian Naive Bayes και MAP estimator\n",
    "\n",
    "Θυμόμαστε ότι:\n",
    "\n",
    "$$\n",
    "P(y=k \\mid x) \\propto P(y=k)\\,P(x \\mid y=k).\n",
    "$$\n",
    "\n",
    "Σε λογαριθμική μορφή:\n",
    "\n",
    "$$\n",
    "\\log P(y=k \\mid x)\n",
    "= \\log P(y=k) + \\log P(x \\mid y=k) + \\text{σταθερά},\n",
    "$$\n",
    "\n",
    "όπου η «σταθερά» δεν εξαρτάται από το $k$ και άρα μπορεί να αγνοηθεί στον υπολογισμό του $\\arg\\max$.\n",
    "\n",
    "Στο Gaussian Naive Bayes αυτό γίνεται:\n",
    "\n",
    "$$\n",
    "\\log P(y=k \\mid x)\n",
    "\\propto\n",
    "\\log \\pi_k\n",
    "+\n",
    "\\sum_{i=1}^d \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2),\n",
    "$$\n",
    "\n",
    "οπότε ο πρακτικός υπολογισμός της log-posterior για την κλάση $k$ είναι:\n",
    "\n",
    "$$\n",
    "\\text{log\\_post}_k\n",
    "= \\log \\pi_k\n",
    "+ \\sum_{i=1}^d \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2).\n",
    "$$\n",
    "\n",
    "Ο εκτιμητής MAP (maximum a posteriori estimator) επιλέγει την κλάση:\n",
    "\n",
    "$$\n",
    "\\hat y(x)\n",
    "= \\arg\\max_k \\text{log\\_post}_k.\n",
    "$$\n",
    "\n",
    "Αυτό είναι ακριβώς αυτό που υλοποιεί η μέθοδος `predict(x)` στο `GaussianNB`.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Από log-posterior σε κανονικές πιθανότητες (log-sum-exp trick και softmax)\n",
    "\n",
    "Για να πάρουμε **κανονικοποιημένες εκ των υστέρων πιθανότητες** (normalized posterior probabilities), ώστε να αθροίζονται στο 1, πρέπει να μετατρέψουμε τις log-τιμές σε «κανονικές» πιθανότητες.\n",
    "\n",
    "Αν έχουμε ένα διάνυσμα log-τιμών:\n",
    "\n",
    "$$\n",
    "z_k = \\text{log\\_post}_k,\n",
    "$$\n",
    "\n",
    "τότε ο απλός ορισμός θα ήταν:\n",
    "\n",
    "$$\n",
    "p_k = \\frac{e^{z_k}}{\\sum_j e^{z_j}}.\n",
    "$$\n",
    "\n",
    "Ωστόσο, για να αποφύγουμε αριθμητικά προβλήματα (overflow/underflow), χρησιμοποιούμε ένα κόλπο αριθμητικής σταθερότητας, γνωστό ως **log-sum-exp trick**:\n",
    "\n",
    "1. Υπολογίζουμε τη μέγιστη log-τιμή:\n",
    "\n",
    "   $$\n",
    "   m = \\max_k z_k.\n",
    "   $$\n",
    "\n",
    "2. «Μετατοπίζουμε» (shift) όλα τα $z_k$:\n",
    "\n",
    "   $$\n",
    "   z'_k = z_k - m.\n",
    "   $$\n",
    "\n",
    "3. Υπολογίζουμε:\n",
    "\n",
    "   $$\n",
    "   p_k = \\frac{e^{z'_k}}{\\sum_j e^{z'_j}}.\n",
    "   $$\n",
    "\n",
    "Επειδή όλες οι πιθανότητες έχουν πολλαπλασιαστεί με τον ίδιο παράγοντα $e^{-m}$, η κανονικοποίηση διορθώνει αυτόν τον παράγοντα και καταλήγουμε σε σωστές πιθανότητες χωρίς αριθμητικά προβλήματα.\n",
    "\n",
    "Αυτός ο μετασχηματισμός είναι ισοδύναμος με τη συνάρτηση **softmax**:\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}(z)_k\n",
    "=\n",
    "\\frac{e^{z_k}}{\\sum_j e^{z_j}},\n",
    "$$\n",
    "\n",
    "απλώς υλοποιημένη με τρόπο ανθεκτικό σε αριθμητικά σφάλματα (numerically stable implementation).\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Βάση λογαρίθμου και ερμηνεία\n",
    "\n",
    "Στο `GaussianNB` (και γενικά στη scikit-learn) χρησιμοποιείται ο **φυσικός λογάριθμος** (natural logarithm, βάση $e$).  \n",
    "Αν αντί για $\\ln$ χρησιμοποιούσαμε $\\log_{10}$, η διαφορά θα ήταν μόνο ένας σταθερός παράγοντας:\n",
    "\n",
    "$$\n",
    "\\log_{10} x = \\frac{1}{\\ln 10} \\ln x.\n",
    "$$\n",
    "\n",
    "Επειδή ο λογάριθμος είναι μονοτονικά αύξουσα συνάρτηση (monotonically increasing function), ο παράγοντας αυτός:\n",
    "\n",
    "- δεν επηρεάζει το ποια κλάση $k$ μεγιστοποιεί το $\\log P(y=k \\mid x)$,\n",
    "- δεν επηρεάζει τις κανονικοποιημένες posterior πιθανότητες μετά την εφαρμογή της softmax.\n",
    "\n",
    "Άρα για τον ταξινομητή η επιλογή βάσης του λογαρίθμου **δεν αλλάζει το αποτέλεσμα**, απλώς αλλάζει την κλίμακα (scale) των log-τιμών.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Τυποποίηση (standardization) και z-scores\n",
    "\n",
    "Συχνά, πριν από την εκπαίδευση του Gaussian Naive Bayes, εφαρμόζουμε **τυποποίηση** (standardization) σε κάθε χαρακτηριστικό (feature) ώστε όλα τα μεγέθη να βρίσκονται στην ίδια κλίμακα.\n",
    "\n",
    "Για κάθε χαρακτηριστικό $x_i$ ορίζουμε το αντίστοιχο z-score:\n",
    "\n",
    "$$\n",
    "z_i = \\frac{x_i - \\mu_i}{\\sigma_i},\n",
    "\\quad\n",
    "\\mu_i = \\frac{1}{n}\\sum_{j=1}^n x_{j,i},\n",
    "\\quad\n",
    "\\sigma_i = \\sqrt{\\frac{1}{n}\\sum_{j=1}^n (x_{j,i}-\\mu_i)^2}.\n",
    "$$\n",
    "\n",
    "Οι τυποποιημένες τιμές (standardized values, **z-scores**):\n",
    "\n",
    "- έχουν δειγματικό μέσο περίπου $E[z_i] \\approx 0$,\n",
    "- και διακύμανση περίπου $\\operatorname{Var}(z_i) \\approx 1$.\n",
    "\n",
    "Σε αυτό το νέο σύστημα συντεταγμένων (z-scale για τα features, log-scale για τις πιθανότητες):\n",
    "\n",
    "- το GaussianNB εκτιμά τις παραμέτρους $\\hat{\\mu}_{k,i}$ και $\\hat{\\sigma}^2_{k,i}$ πάνω σε **z-scores**,\n",
    "- η log-likelihood γράφεται στη λογαριθμική κλίμακα (log-scale),\n",
    "- και οι αποφάσεις παίρνονται με βάση τον MAP estimator πάνω στις log-posteriors.\n",
    "\n",
    "Πολύ σημαντικό για να αποφύγουμε **διαρροή πληροφορίας** (data leakage):\n",
    "\n",
    "- τα $\\hat{\\mu}_i$ και $\\hat{\\sigma}_i$ του StandardScaler υπολογίζονται **μόνο** στο σύνολο εκπαίδευσης (training set),\n",
    "- ο ίδιος μετασχηματισμός εφαρμόζεται στη συνέχεια στα σύνολα επικύρωσης και ελέγχου (validation/test sets).\n",
    "\n",
    "Έτσι:\n",
    "\n",
    "- κάθε χαρακτηριστικό συμβάλλει ισότιμα στην πιθανοφάνεια (likelihood),\n",
    "- οι Γκαουσιανές (Gaussians) δεν αλλοιώνονται από ετερογενείς μονάδες μέτρησης,\n",
    "- και η χρήση της λογαριθμικής κλίμακας (log-scale) εξασφαλίζει αριθμητική σταθερότητα και καθαρή θεωρητική ερμηνεία των εκ των υστέρων πιθανοτήτων (posterior probabilities).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Το Iris dataset και τα χαρακτηριστικά του (Iris dataset & features)\n",
    "\n",
    "Το Iris dataset αποτελεί ένα κλασικό σύνολο δεδομένων για επίδειξη τεχνικών ταξινόμησης. Περιλαμβάνει μετρήσεις από άνθη τριών ειδών Iris και κάθε εγγραφή (γραμμή) του dataset αντιστοιχεί σε ένα μεμονωμένο δείγμα άνθους (sample / observation).\n",
    "\n",
    "Συνοπτική δομή:\n",
    "\n",
    "- Γραμμές (rows / samples): κάθε γραμμή είναι ένα άνθος (π.χ. ένα δείγμα), και στο Iris dataset έχουμε συνολικά 150 δείγματα.\n",
    "- Στήλες (columns / features): περιλαμβάνουν τις αριθμητικές μετρήσεις που χρησιμοποιούνται ως είσοδοι (features) στον ταξινομητή, καθώς και την στήλη-ετικέτα (label / target):\n",
    "  - `sepal length (cm)` — μήκος του sepal σε εκατοστά (centimeters).\n",
    "  - `sepal width (cm)` — πλάτος του sepal σε εκατοστά.\n",
    "  - `petal length (cm)` — μήκος του petal σε εκατοστά.\n",
    "  - `petal width (cm)` — πλάτος του petal σε εκατοστά.\n",
    "  - `species` (target / label) — είδος του άνθους: `setosa`, `versicolor`, `virginica`.\n",
    "\n",
    "Βιολογική/φυσική σημασία των χαρακτηριστικών:\n",
    "\n",
    "- Το **sepal** (σέπαλο) είναι η εξωτερική δομή που προστατεύει το μπουμπούκι (flower bud). Το **petal** (πέταλο) είναι συνήθως χρωματιστή δομή που προσελκύει επικονιαστές (insects / pollinators) και σχετίζεται με αναπαραγωγικούς μηχανισμούς.\n",
    "\n",
    "- Τα **petal features** (μήκος/πλάτος πέταλου) συχνά φέρουν περισσότερη πληροφορία για τη διαφορετικότητα των ειδών λόγω διαφοροποίησης που σχετίζεται με επικονιαστές και λειτουργία άνθισης. Γι' αυτό στα δεδομένα του Iris συνήθως αποτελούν τα πιο διαχωριστικά χαρακτηριστικά.\n",
    "\n",
    "- Τα **sepal features** (μήκος/πλάτος σέπαλου) εμφανίζουν συχνά μικρότερη διασπορά ανάμεσα στα είδη και μπορεί να είναι λιγότερο διακριτικά.\n",
    "\n",
    "Συμβουλές κατά την ανάλυση:\n",
    "\n",
    "- Επιβεβαίωσε ότι οι μονάδες (centimeters) έχουν ενιαία μορφή και ελέγξτε για missing values πριν την ανάλυση (π.χ. `df.isnull().sum()`).\n",
    "- Προτού εκπαιδεύσετε μοντέλα, συχνά εφαρμόζουμε **τυποποίηση (StandardScaler)** ώστε τα χαρακτηριστικά σε διαφορετικές κλίμακες να γίνουν συγκρίσιμα (μηδενικός μέσος, μονάδα τυπικής απόκλισης).\n",
    "\n",
    "Σύνδεση με ταξινόμηση και GaussNB:\n",
    "\n",
    "- Τα χαρακτηριστικά χρησιμοποιούνται ως `X` (features), ενώ η στήλη `species` ως `y` (target/label).\n",
    "- Το Gaussian Naive Bayes (GaussianNB) χρησιμοποιεί στατιστικές παραμέτρους (μέσους / διακυμάνσεις ανά κλάση) για να μοντελοποιήσει τις πιθανότητες P(x | y) και να υπολογίσει την εκ των υστέρων πιθανότητα (posterior) P(y | x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Βασικά imports: εισάγουμε τις βιβλιοθήκες που θα χρησιμοποιήσουμε\n",
    "# numpy: αριθμητικές πράξεις και πίνακες\n",
    "import numpy as np\n",
    "# matplotlib: plotting/γραφικές παραστάσεις\n",
    "import matplotlib\n",
    "# Force inline backend to avoid VSCode Plot Viewer duplicates and ensure inline rendering\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# scikit-learn: φορτώνουμε το iris dataset, κάνουμε split, scaling και GaussianNB\n",
    "from sklearn.datasets import load_iris  # φορτώνει το κλασικό Iris dataset\n",
    "from sklearn.model_selection import train_test_split  # χωρισμός train/validation\n",
    "from sklearn.preprocessing import StandardScaler  # κανονικοποίηση/standardization\n",
    "from sklearn.naive_bayes import GaussianNB  # Gaussian Naive Bayes classifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay  # μετρικές αξιολόγησης\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Φόρτωση του Iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Recompute canonical colormap now that we know class_names length\n",
    "cmap = plt.get_cmap('tab10')\n",
    "# Create discrete colormap and normalization for class integers (0..K-1)\n",
    "K = len(class_names)\n",
    "class_colors = [cmap(i) for i in range(K)]\n",
    "class_cmap = ListedColormap(class_colors)\n",
    "class_norm = BoundaryNorm(np.arange(K + 1) - 0.5, K)\n",
    "# Also keep CLASS_COLORS as hex list for textual usage and CLASS_COLOR_MAP mapping\n",
    "CLASS_COLORS = [mcolors.to_hex(c) for c in class_colors]\n",
    "CLASS_COLOR_MAP = dict(zip(class_names, CLASS_COLORS))\n",
    "# Optionally disable static scatter plots if you prefer a single interactive figure (set to True/False)\n",
    "\n",
    "print(\"Σχήμα X:\", X.shape)\n",
    "print(\"Χαρακτηριστικά:\", feature_names)\n",
    "print(\"Κλάσεις:\", class_names)\n",
    "print('CLASS_COLORS =', CLASS_COLORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ρίχνουμε μια πρώτη ματιά στα δεδομένα (πρώτα 5 δείγματα)\n",
    "\n",
    "X[:5], y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Απλή 2D απεικόνιση: petal length vs petal width\n",
    "# Δημιουργούμε ένα figure και ένα axis για να σχεδιάσουμε\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Σχεδιάζουμε scatter plot: X[:, 2] = petal length, X[:, 3] = petal width\n",
    "# Use discrete class colormap (class_cmap) with class_norm to map integer classes consistently\n",
    "scatter = ax.scatter(X[:, 2], X[:, 3], c=y, cmap=class_cmap, norm=class_norm, alpha=0.8)\n",
    "\n",
    "# Δημιουργία απλού legend (Line2D handles with CLASS_COLORS)\n",
    "from matplotlib.lines import Line2D\n",
    "legend_colors = CLASS_COLORS\n",
    "handles = [Line2D([0], [0], marker='o', color=legend_colors[i], linestyle='', markersize=8) for i in range(len(class_names))]\n",
    "ax.legend(handles, class_names, title='Class')  # π.χ. setosa, versicolor, virginica\n",
    "\n",
    "# Βάζουμε ετικέτες στους άξονες και τίτλο\n",
    "ax.set_xlabel(\"petal length (cm)\")  # μήκος πετάλου\n",
    "ax.set_ylabel(\"petal width (cm)\")   # πλάτος πετάλου\n",
    "ax.set_title(\"Iris – petal length vs petal width\")\n",
    "\n",
    "# Κάνουμε tight layout για καλύτερη εμφάνιση\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Ερμηνεία scatter plot (petal length vs petal width)\n",
    "\n",
    "- Άξονες:\n",
    "  - Οριζόντιος άξονας: `petal length (cm)` — μήκος πετάλου σε εκατοστά.\n",
    "  - Κατακόρυφος άξονας: `petal width (cm)` — πλάτος πετάλου σε εκατοστά.\n",
    "\n",
    "- Κωδικοποίηση δεδομένων:\n",
    "  - Κάθε σημείο αντιπροσωπεύει ένα δείγμα (παρατήρηση / sample) άνθους.\n",
    "  - Το χρώμα δείχνει την πραγματική κλάση (species / target): `setosa`, `versicolor`, `virginica`.\n",
    "  - Το legend (υπόμνημα) στο διάγραμμα δείχνει ποιο χρώμα αντιστοιχεί σε ποια κλάση.\n",
    "\n",
    "Σκοπός του διαγράμματος:\n",
    "- Ελέγχουμε τη **διαχωρισιμότητα (separability)** των κλάσεων στον χώρο των χαρακτηριστικών που έχει μεγαλύτερη πληροφορία: τα χαρακτηριστικά `petal length/width` συχνά παρέχουν ισχυρή διάκριση μεταξύ των ειδών.\n",
    "\n",
    "Τι παρατηρούμε :\n",
    "1. Η κλάση `Iris setosa` εμφανίζεται ως ξεκάθαρο, απομονωμένο cluster  με μικρές τιμές petal length/width — εξηγεί γιατί ο ταξινομητής προβλέπει setosa με πολύ υψηλή ακρίβεια (precision & recall).\n",
    "2. Οι κλάσεις `Iris versicolor` και `Iris virginica` παρουσιάζουν μερική επικάλυψη (overlap) στα petal features — γι' αυτό παρουσιάζονται περισσότερες λάθος εκτιμήσεις (misclassifications) ανάμεσά τους.\n",
    "\n",
    "Σύνδεση με μοντέλο (model connection):\n",
    "- Το διάγραμμα δείχνει γιατί ένα απλό μοντέλο όπως το GaussianNB μπορεί να διαχωρίσει εύκολα την `setosa` αλλά να κάνει λάθη ανάμεσα σε `versicolor` και `virginica` στις περιοχές όπου οι κατανομές επικαλύπτονται.\n",
    "- Στα σημεία επικάλυψης η **εκ των υστέρων πιθανότητα (posterior probability)** είναι πιο ισορροπημένη (ελάχιστα διακριτή) και το `predict_proba()` θα δείξει ανάλογα μικρή βεβαιότητα.\n",
    "\n",
    "Σημεία σχολιασμού :\n",
    "- Οι άξονες προβάλλονται σε φυσικές μονάδες (cm), αλλά πριν την εκπαίδευση χρησιμοποιούμε συχνά **τυποποίηση (standardization)** ώστε να συγκρίνονται τα χαρακτηριστικά.\n",
    "- Το διάγραμμα είναι χρήσιμο για επιλογή χαρακτηριστικών (feature selection) και για την ερμηνεία των σφαλμάτων που παρατηρούμε στο confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εξερεύνηση: Boxplots για κάθε χαρακτηριστικό ανά κλάση\n",
    "# Χρησιμοποιούμε pandas για εύκολο grouping και plotting\n",
    "import pandas as pd\n",
    "\n",
    "# Δημιουργία DataFrame με ονόματα χαρακτηριστικών\n",
    "iris_df = pd.DataFrame(X, columns=feature_names)\n",
    "iris_df['species'] = [class_names[idx] for idx in y]\n",
    "\n",
    "# Εξερεύνηση: Boxplots για κάθε χαρακτηριστικό ανά κλάση — απλή και φιλική για αρχάριους\n",
    "# Χρησιμοποιούμε Matplotlib.boxplot with colors derived from tab10 colormap\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "for i, feat in enumerate(feature_names):\n",
    "    # Δεδομένα: μια λίστα με τις τιμές κάθε κλάσης για το χαρακτηριστικό feat\n",
    "    data_per_class = [iris_df[iris_df['species'] == cls][feat].values for cls in class_names]\n",
    "    # Σχεδιάζουμε grouped boxplot — κάθε στοιχείο του data_per_class είναι μια ομάδα\n",
    "    bplot = axes[i].boxplot(data_per_class, tick_labels=class_names, patch_artist=True)\n",
    "    # Ορίζουμε τα χρώματα των κουτιών (boxes) με βάση το class_colors\n",
    "    for patch, color in zip(bplot['boxes'], class_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_edgecolor('k')\n",
    "    axes[i].set_title(feat)\n",
    "    axes[i].set_xlabel('')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εξερεύνηση: pairwise scatterplots για μερικούς συνδυασμούς χαρακτηριστικών\n",
    "# Επιλέγουμε μερικά ζεύγη χαρακτηριστικών για να ελέγξουμε συμμετρία/συσχέτιση\n",
    "pairs = [\n",
    "    ('sepal length (cm)', 'sepal width (cm)'),\n",
    "    ('petal length (cm)', 'petal width (cm)'),\n",
    "    ('sepal length (cm)', 'petal length (cm)'),\n",
    "    ('sepal width (cm)', 'petal width (cm)'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Σχεδιάζουμε κάθε ζεύγος σε ξεχωριστό subplot και χρωματίζουμε ανά species\n",
    "for ax, (xcol, ycol) in zip(axes.flatten(), pairs):\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        sub = iris_df[iris_df['species'] == cls_name]\n",
    "        # Use discrete class colormap and norm; c list maps to integer class code\n",
    "        cvals = np.array([cls_idx] * len(sub))\n",
    "        ax.scatter(sub[xcol], sub[ycol], label=cls_name, alpha=0.7, c=cvals, cmap=class_cmap, norm=class_norm)\n",
    "    ax.set_xlabel(xcol)\n",
    "    ax.set_ylabel(ycol)\n",
    "    ax.set_title(f\"{xcol} vs {ycol}\")\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εξερεύνηση: mean ± std ανά κλάση για κάθε χαρακτηριστικό\n",
    "# Υπολογίζουμε τον μέσο και την τυπική απόκλιση για κάθε species\n",
    "means = iris_df.groupby('species')[feature_names].mean()\n",
    "stds = iris_df.groupby('species')[feature_names].std()\n",
    "\n",
    "# Σχεδιάζουμε 4 subplots, ένα για κάθε χαρακτηριστικό\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for ax, feat in zip(axes.flatten(), feature_names):\n",
    "    # plot mean with errorbars (std) for each species\n",
    "    x = np.arange(len(means.index))\n",
    "    ax.bar(x, means[feat], yerr=stds[feat], capsize=6, color=CLASS_COLORS)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(means.index)\n",
    "    ax.set_title(f\"{feat} mean ± std by species\")\n",
    "    ax.set_ylabel(feat)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / validation split και κανονικοποίηση με StandardScaler\n",
    "# Χωρίζουμε το dataset σε train και validation ώστε να αξιολογήσουμε την απόδοση σε κρατημένα δεδομένα\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,  # κρατάμε την αναλογία των κλάσεων σε train & val\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Δημιουργούμε τον StandardScaler και τον εφαρμόζουμε μόνο στο training set\n",
    "# ΣΗΜΑΝΤΙΚΟ: ποτέ δεν πρέπει να βασίσουμε τον scaler στο συνολικό dataset (data leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # fit μόνο στο train\n",
    "X_val_scaled = scaler.transform(X_val)  # transform στο val με το scaler που μάθαμε από το train\n",
    "\n",
    "# Εκπαίδευση GaussianNB (μονάδα scikit-learn)\n",
    "# Η GaussianNB υποθέτει κανονικές κατανομές για κάθε χαρακτηριστικό ανά κλάση\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Κάνουμε προβλέψεις στο validation set\n",
    "y_pred = gnb.predict(X_val_scaled)\n",
    "\n",
    "# Εκτυπώνουμε την αναφορά ταξινόμησης (precision, recall, f1) για κάθε κλάση\n",
    "print(\"=== Gaussian Naive Bayes στο Iris dataset ===\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_val,\n",
    "        y_pred,\n",
    "        target_names=class_names,\n",
    "        digits=3,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Απλή και φιλική παρουσίαση των παραμέτρων που έμαθε το GaussianNB\n",
    "# Θεωρούμε ότι τα gnb, class_names και feature_names έχουν οριστεί σε προηγούμενα cells\n",
    "\n",
    "feature_names_obj = feature_names if 'feature_names' in globals() else [f'feature_{i}' for i in range(gnb.theta_.shape[1])]\n",
    "class_names_obj = class_names\n",
    "\n",
    "# Περίληψη: πλήθος και empirical priors\n",
    "summary_df = pd.DataFrame({\n",
    "    'Κλάση': class_names_obj,\n",
    "    'N_k': gnb.class_count_.astype(int),\n",
    "    'Empirical prior': gnb.class_prior_\n",
    "})\n",
    "\n",
    "# Means and variances per-class (readable with column names)\n",
    "means_df = pd.DataFrame(gnb.theta_, index=class_names_obj, columns=feature_names_obj)\n",
    "means_df.index.name = 'Κλάση'\n",
    "vars_df = pd.DataFrame(gnb.var_, index=class_names_obj, columns=feature_names_obj)\n",
    "vars_df.index.name = 'Κλάση'\n",
    "\n",
    "\n",
    "# Display in beginner-friendly format\n",
    "print('=== GaussianNB learned parameters ===')\n",
    "print()\n",
    "display(summary_df)\n",
    "print('\\nΜέσοι ανά κλάση (theta_)')\n",
    "display(means_df)\n",
    "print('\\nΔιακυμάνσεις ανά κλάση (var_)')\n",
    "display(vars_df)\n",
    "\n",
    "# Show shapes and a short note\n",
    "print('\\nShapes:')\n",
    "print('  theta_.shape =', gnb.theta_.shape)\n",
    "print('  var_.shape   =', gnb.var_.shape)\n",
    "print('\\nΣημείωση: Οι παραπάνω τιμές αφορούν τυποποιημένα χαρακτηριστικά (StandardScaler).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Σύνδεση των παραμέτρων που μάθαμε (learned parameters) με τη θεωρία Gaussian Naive Bayes\n",
    "\n",
    "Σε αυτό το κελί ερμηνεύουμε τις αριθμητικές τιμές που έμαθε το `GaussianNB` και πώς αυτές μπαίνουν στους θεωρητικούς τύπους για την εκ των υστέρων πιθανότητα (posterior probability) και τον MAP ταξινομητή.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Εμπειρικές εκ των προτέρων πιθανότητες (empirical class priors)\n",
    "\n",
    "Από την εκπαίδευση πήραμε:\n",
    "\n",
    "- `class_count_ = [40., 40., 40.]`  \n",
    "  ⇒ για κάθε κλάση $k$ ισχύει $N_k = 40$ και συνολικά $N = 120$.\n",
    "- Άρα:\n",
    "  $$\n",
    "  \\hat{\\pi}_k\n",
    "  = \\frac{N_k}{N}\n",
    "  = \\frac{40}{120}\n",
    "  = \\frac{1}{3},\n",
    "  $$\n",
    "  που φαίνεται και στο `class_prior_ = [1/3, 1/3, 1/3]`.\n",
    "\n",
    "Ερμηνεία: το prior $P(y = k)$ είναι **ουδέτερο** (uniform) και δεν προτιμά κάποια κλάση πριν δούμε τα χαρακτηριστικά $x$. Στον τύπο του Bayes αυτό είναι ο όρος:\n",
    "\n",
    "$$\n",
    "\\log P(y=k) = \\log \\pi_k.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Μέσοι και διακυμάνσεις ανά κλάση (Gaussian parameters)\n",
    "\n",
    "Οι πίνακες `theta_` και `var_` δίνουν τους μέσους (means) και τις διακυμάνσεις (variances) των χαρακτηριστικών μετά την τυποποίηση (StandardScaler), για κάθε κλάση:\n",
    "\n",
    "- **setosa**  \n",
    "  - μέσοι (means) για τα χαρακτηριστικά:\n",
    "    - sepal: περίπου [-1.00, 0.82],\n",
    "    - petal: περίπου [-1.30, -1.26],\n",
    "  - διακυμάνσεις (variances) στα petal features: περίπου [0.0096, 0.0197] (πολύ μικρές).\n",
    "\n",
    "- **versicolor**  \n",
    "  - μέσοι: περίπου [0.05, -0.67, 0.27, 0.17],\n",
    "  - διακυμάνσεις: περίπου [0.34, 0.53, 0.077, 0.069].\n",
    "\n",
    "- **virginica**  \n",
    "  - μέσοι: περίπου [0.95, -0.14, 1.03, 1.09],\n",
    "  - διακυμάνσεις: περίπου [0.56, 0.54, 0.107, 0.115].\n",
    "\n",
    "Θεωρητικά, αυτά είναι ακριβώς τα $\\hat{\\mu}_{k,i}$ και $\\hat{\\sigma}^2_{k,i}$ στην Κανονική κατανομή (Gaussian pdf):\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y = k)\n",
    "= \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2)\n",
    "= \\frac{1}{\\sqrt{2\\pi\\sigma_{k,i}^2}}\n",
    "  \\exp\\left(-\\frac{(x_i-\\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right).\n",
    "$$\n",
    "\n",
    "Ιδιαίτερα για τη **setosa** βλέπουμε:\n",
    "\n",
    "- τα petal features έχουν μέσους γύρω στο -1.3,\n",
    "- με εξαιρετικά μικρές διακυμάνσεις,\n",
    "- άρα η pdf είναι πολύ «στενή» γύρω από τον μέσο ⇒ δείγματα κοντά σε αυτές τις τιμές έχουν πολύ υψηλή πιθανοφάνεια (likelihood) υπέρ setosa.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Πώς μπαίνουν οι παράμετροι στη log-πιθανοφάνεια (log-likelihood)\n",
    "\n",
    "Για ένα δείγμα $x = (x_1, \\dots, x_d)$ και συγκεκριμένη κλάση $k$, η συνολική (συνεχής) πιθανοφάνεια είναι:\n",
    "\n",
    "$$\n",
    "P(x \\mid y = k)\n",
    "= \\prod_{i=1}^d \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2).\n",
    "$$\n",
    "\n",
    "Σε λογαριθμική κλίμακα (log-scale), που είναι αυτή που χρησιμοποιεί ο `GaussianNB`, έχουμε:\n",
    "\n",
    "$$\n",
    "\\log P(x \\mid y = k)\n",
    "= \\sum_{i=1}^d \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2).\n",
    "$$\n",
    "\n",
    "Κάθε όρος \n",
    "$\n",
    "\\log \\mathcal{N}(\\cdot)\n",
    "$\n",
    "υπολογίζεται από τα αντίστοιχα `theta_[k, i]` και `var_[k, i]`:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2)\n",
    "= -\\frac{1}{2}\n",
    "\\left[\n",
    "\\log(2\\pi\\sigma_{k,i}^2)\n",
    "+\n",
    "\\frac{(x_i - \\mu_{k,i})^2}{\\sigma_{k,i}^2}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Άρα οι πίνακες `theta_` και `var_` «μπαίνουν» κατευθείαν μέσα στη log-pdf.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Log-posterior και MAP απόφαση (σύμφωνα με τη θεωρία)\n",
    "\n",
    "Ο τύπος της εκ των υστέρων πιθανότητας (posterior probability) σε log-scale είναι:\n",
    "\n",
    "$$\n",
    "\\log P(y=k \\mid x)\n",
    "\\propto\n",
    "\\log \\pi_k\n",
    "+\n",
    "\\sum_{i=1}^d \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma_{k,i}^2).\n",
    "$$\n",
    "\n",
    "Αυτό που υπολογίζει εσωτερικά το `GaussianNB` είναι ουσιαστικά:\n",
    "\n",
    "$$\n",
    "\\text{log\\_post}_k\n",
    "= \\log \\pi_k\n",
    "+ \\sum_{i=1}^d \\log \\mathcal{N}(x_i \\mid \\hat{\\mu}_{k,i}, \\hat{\\sigma}_{k,i}^2),\n",
    "$$\n",
    "\n",
    "και ο ταξινομητής επιλέγει την κλάση με μέγιστη log-posterior (MAP estimator):\n",
    "\n",
    "$$\n",
    "\\hat{y}(x)\n",
    "= \\arg\\max_k \\text{log\\_post}_k.\n",
    "$$\n",
    "\n",
    "Η πολύ μικρή διακύμανση στα petal χαρακτηριστικά της `setosa` σημαίνει ότι, όταν ένα δείγμα έχει petal length/width κοντά στις τιμές `[-1.3, -1.26]`, οι log-pdfs αυτών των χαρακτηριστικών για τη setosa γίνονται πολύ μεγαλύτερες από τις αντίστοιχες των άλλων κλάσεων, άρα:\n",
    "\n",
    "- το $\\log P(x \\mid y=\\text{setosa})$ είναι πολύ μεγαλύτερο,\n",
    "- και έτσι το $\\log P(y=\\text{setosa} \\mid x)$ κυριαρχεί,\n",
    "- οπότε η πρόβλεψη είναι σχεδόν βέβαια `setosa`.\n",
    "\n",
    "Αντίστοιχα:\n",
    "\n",
    "- τιμές petal κοντά στο ~0.3 ευνοούν `versicolor`,\n",
    "- τιμές γύρω στο ~1.0 ευνοούν `virginica`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ρόλος της τυποποίησης (StandardScaler) και z-scores\n",
    "\n",
    "Θυμόμαστε ότι:\n",
    "\n",
    "- εφαρμόσαμε `StandardScaler` **μόνο στο training set** (`fit_transform`),\n",
    "- και στη συνέχεια χρησιμοποιήσαμε τον ίδιο scaler στο validation set (`transform`),\n",
    "- αποφεύγοντας data leakage.\n",
    "\n",
    "Γι’ αυτό οι μέσοι και οι διακυμάνσεις που βλέπουμε (`theta_`, `var_`) αναφέρονται σε **τυποποιημένα χαρακτηριστικά** (z-scores):\n",
    "\n",
    "$$\n",
    "z_i = \\frac{x_i - \\mu_i}{\\sigma_i}.\n",
    "$$\n",
    "\n",
    "Σε αυτό το z-scale:\n",
    "\n",
    "- οι «φυσικές» μονάδες (cm) έχουν αφαιρεθεί,\n",
    "- οι κλάσεις διαχωρίζονται πάνω σε μια κοινή κλίμακα,\n",
    "- και το GaussianNB μαθαίνει Γκαουσιανές πάνω στο κανονικοποιημένο χώρο, όπως ακριβώς περιγράφει η θεωρία.\n",
    "\n",
    "Έτσι, τα αποτελέσματα του `classification_report` (υψηλά precision/recall για όλες τις κλάσεις και συνολική accuracy ≈ 0.967) συμφωνούν με την εικόνα που δίνουν οι παράμετροι:  \n",
    "οι κλάσεις στο Iris είναι καλά διαχωρίσιμες και το Gaussian Naive Bayes, με τα σωστά priors και τις Γκαουσιανές πάνω στα z-scores, τις διακρίνει πολύ αποτελεσματικά.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - Σύγκριση κλασσικής μορφής Bayes (γινόμενα pdf) με log-scale υλοποίηση\n",
    "# Εδώ υπολογίζουμε τους joint scores P(y=k) * Π_i P(x_i | y=k) σε σκέτη μορφή\n",
    "# και δείχνουμε ότι, μετά από κανονικοποίηση, δίνουν τις ίδιες posterior με την log-based μέθοδο.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "required_vars = ['gnb', 'X_val_scaled', 'class_names']\n",
    "missing = [name for name in required_vars if name not in globals()]\n",
    "if missing:\n",
    "    print(\"Σφάλμα: Τρέξτε πρώτα τα προηγούμενα cells ώστε να οριστούν \" + \", \".join(missing))\n",
    "else:\n",
    "    # Gaussian pdf (όχι log) για ένα feature\n",
    "    def gaussian_pdf(x, mean, var):\n",
    "        return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(-0.5 * ((x - mean) ** 2) / var)\n",
    "\n",
    "    idx = 0\n",
    "    x_scaled = X_val_scaled[idx]\n",
    "\n",
    "    raw_joints = []\n",
    "    per_class_feature_pdfs = []\n",
    "\n",
    "    for k, cls in enumerate(class_names):\n",
    "        prior_k = gnb.class_prior_[k]\n",
    "        feature_pdfs = []\n",
    "        for i in range(x_scaled.shape[0]):\n",
    "            mean = gnb.theta_[k, i]\n",
    "            var = gnb.var_[k, i]\n",
    "            pdf_i = gaussian_pdf(x_scaled[i], mean, var)\n",
    "            feature_pdfs.append(pdf_i)\n",
    "        feature_pdfs = np.array(feature_pdfs)\n",
    "        per_class_feature_pdfs.append(feature_pdfs)\n",
    "\n",
    "        product_likelihood = feature_pdfs.prod()         # Π_i P(x_i | y=k)\n",
    "        raw_joint = prior_k * product_likelihood         # P(y=k) * Π_i P(x_i | y=k)\n",
    "        raw_joints.append(raw_joint)\n",
    "\n",
    "    raw_joints = np.array(raw_joints)\n",
    "    normalized_post_from_raw = raw_joints / raw_joints.sum()\n",
    "\n",
    "    print(f\"Raw unnormalized joint scores P(y=k)*Π_i P(x_i|y=k) για sample idx {idx}:\")\n",
    "    for k, cls in enumerate(class_names):\n",
    "        print(f\"  {cls}: raw_joint={raw_joints[k]:.6e}, normalized posterior ≈ {normalized_post_from_raw[k]:.6f}\")\n",
    "\n",
    "    # Σύγκριση με predict_proba (log-based υλοποίηση του GaussianNB)\n",
    "    proba = gnb.predict_proba(x_scaled.reshape(1, -1))[0]\n",
    "    print(\"\\nPosterior από GaussianNB (predict_proba):\")\n",
    "    for k, cls in enumerate(class_names):\n",
    "        print(f\"  {cls}: {proba[k]:.6f}\")\n",
    "\n",
    "    print(\"\\nPer-feature likelihoods P(x_i | y=k):\")\n",
    "    for k, cls in enumerate(class_names):\n",
    "        print(f\"  {cls}: {np.round(per_class_feature_pdfs[k], 6)}\")\n",
    "\n",
    "    # Προαιρετικά: δείχνουμε και τα log-joints για να συνδέσουμε τη μορφή log_prior + Σ log_pdf\n",
    "    log_joints = []\n",
    "    for k, cls in enumerate(class_names):\n",
    "        log_prior_k = np.log(gnb.class_prior_[k])\n",
    "        log_likelihood_sum = 0.0\n",
    "        for i in range(x_scaled.shape[0]):\n",
    "            mean = gnb.theta_[k, i]\n",
    "            var = gnb.var_[k, i]\n",
    "            log_likelihood_sum += -0.5 * (np.log(2 * np.pi * var) + ((x_scaled[i] - mean) ** 2) / var)\n",
    "        log_joints.append(log_prior_k + log_likelihood_sum)\n",
    "    log_joints = np.array(log_joints)\n",
    "\n",
    "    log_shift = log_joints - log_joints.max()\n",
    "    log_normalized = np.exp(log_shift) / np.exp(log_shift).sum()\n",
    "\n",
    "    print(\"\\nLog-joints (log P(y=k) + Σ_i log P(x_i | y=k)):\", np.round(log_joints, 6))\n",
    "    print(\"Posterior από log-based normalization          :\", np.round(log_normalized, 6))\n",
    "\n",
    "    print(\"\\nΔίδαγμα: η κλασσική μορφή με γινόμενα pdf και η log-based μορφή είναι μαθηματικά ισοδύναμες,\")\n",
    "    print(\"αλλά η log-scale υλοποίηση είναι αριθμητικά πολύ πιο σταθερή για πολλά features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Argmax των εκ των υστέρων πιθανοτήτων (posterior) και `predict()` στο GaussianNB\n",
    "\n",
    "Στα προηγούμενα κελιά είδαμε ότι μπορούμε να υπολογίσουμε για κάθε κλάση $k$ την εκ των υστέρων πιθανότητα\n",
    "(posterior probability) $P(y=k \\mid x)$ είτε στην κλασική μορφή\n",
    "\n",
    "$$\n",
    "P(y=k \\mid x)\n",
    "= \\frac{P(y=k) \\; P(x \\mid y=k)}{\\sum_j P(y=j) \\; P(x \\mid y=j)},\n",
    "$$\n",
    "\n",
    "είτε σε λογαριθμική κλίμακα (log-scale) ως\n",
    "\n",
    "$$\n",
    "\\log P(y=k \\mid x)\n",
    "\\propto\n",
    "\\log \\pi_k\n",
    "+\n",
    "\\sum_i \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma^2_{k,i}).\n",
    "$$\n",
    "\n",
    "Ο ταξινομητής Gaussian Naive Bayes (GaussianNB) δεν χρειάζεται να επιστρέψει την πλήρη κατανομή — για την πρόβλεψη\n",
    "της κλάσης αρκεί να βρούμε ποιο $k$ μεγιστοποιεί την posterior. Αυτό γίνεται με τον τελεστή **argmax**:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x)\n",
    "=\n",
    "\\arg\\max_k \\; \\log P(y=k \\mid x)\n",
    "=\n",
    "\\arg\\max_k \\Big( \\log \\pi_k + \\sum_i \\log \\mathcal{N}(x_i \\mid \\mu_{k,i}, \\sigma^2_{k,i}) \\Big).\n",
    "$$\n",
    "\n",
    "Στην πράξη:\n",
    "\n",
    "- η μέθοδος `predict_proba(x)` του `GaussianNB` υπολογίζει τις κανονικοποιημένες εκ των υστέρων πιθανότητες\n",
    "  (posterior probabilities) $P(y=k \\mid x)$,\n",
    "- η μέθοδος `predict(x)` εφαρμόζει απλώς `argmax` επάνω σε αυτές τις πιθανότητες (ή στα log-posteriors)\n",
    "  και επιστρέφει την κλάση με τη μεγαλύτερη τιμή.\n",
    "\n",
    "Για το δείγμα `idx=0` στο Iris dataset, από τον χειροκίνητο υπολογισμό βρήκαμε log-posteriors π.χ.\n",
    "\n",
    "$$\n",
    "\\text{log\\_post} = [-0.935, -37.949, -57.333],\n",
    "$$\n",
    "\n",
    "οπότε ο argmax επιλέγει την πρώτη κλάση (`setosa`).  \n",
    "Η σύγκριση με το `predict_proba` και το `predict` του `GaussianNB` έδειξε ότι:\n",
    "\n",
    "- οι χειροκίνητα υπολογισμένες posterior $P(y=k \\mid x)$ ταιριάζουν με αυτές του μοντέλου,\n",
    "- το `predict` συμφωνεί με τον argmax επάνω στις log-posteriors.\n",
    "\n",
    "**Δίδαγμα:** ο GaussianNB υλοποιεί ακριβώς τον θεωρητικό κανόνα του Bayes με Γκαουσιανές pdfs και επιστρέφει ως\n",
    "πρόβλεψη την κλάση με μέγιστη εκ των υστέρων πιθανότητα (MAP estimator). Οι μικρές διακυμάνσεις (variances) της\n",
    "`setosa` κάνουν τις αντίστοιχες log-likelihoods πολύ μεγάλες κοντά στους μέσους της, άρα και το posterior για\n",
    "`setosa` κυριαρχεί σε αυτά τα σημεία του χώρου χαρακτηριστικών.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Οπτική απεικόνιση: Gaussian PDFs (scaled) για μήκος πετάλου (petal length) ανά κλάση\n",
    "# Εδώ σχεδιάζουμε τις per-class likelihood functions P(x_i | y=k) για ένα feature (petal length)\n",
    "# Αυτές οι συναρτήσεις χρησιμοποιούνται στον κανόνα του Bayes ως πολλαπλασιαστές για κάθε χαρακτηριστικό:\n",
    "#   P(x | y=k) = Π_i P(x_i | y=k), εδώ βλέπουμε το συστατικό P(petal_length | y=k)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'gnb' not in globals() or 'X_train_scaled' not in globals():\n",
    "    print('Σφάλμα: Τρέξτε πρώτα το training cell ώστε να οριστούν gnb και X_train_scaled')\n",
    "else:\n",
    "    feat_idx = 2  # petal length index\n",
    "    means = gnb.theta_[:, feat_idx]\n",
    "    vars_ = gnb.var_[:, feat_idx]\n",
    "\n",
    "    # Range for x (scaled units)\n",
    "    x_min, x_max = X_train_scaled[:, feat_idx].min() - 0.5, X_train_scaled[:, feat_idx].max() + 0.5\n",
    "    xs = np.linspace(x_min, x_max, 400)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Plot histogram of training petal length values (scaled)\n",
    "    plt.hist(X_train_scaled[:, feat_idx], bins=30, density=True, alpha=0.35, label='train petal length (scaled)')\n",
    "\n",
    "    for k, cls in enumerate(class_names):\n",
    "        mu = means[k]\n",
    "        var = vars_[k]\n",
    "        # pdf of Gaussian: P(x | y=k) = (1 / sqrt(2π var)) * exp(-0.5 * ((x - mu)^2) / var)\n",
    "        pdf = (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(-0.5 * ((xs - mu) ** 2) / var)\n",
    "        # plot the per-class likelihood for the feature (one ingredient of Π_i P(x_i | y=k))\n",
    "        plt.plot(xs, pdf, color=CLASS_COLORS[k], lw=2, label=f'{cls} Gaussian')\n",
    "        plt.axvline(mu, color=CLASS_COLORS[k], ls='--', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('petal length (scaled)')\n",
    "    plt.title('Per-class Gaussian PDFs for petal length (scaled) — these are the per-feature likelihoods P(x_i | y=k)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Απλό interactive demo: posterior & predict με petal length / width\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"ipywidgets\") is None:\n",
    "    print(\"ipywidgets δεν είναι εγκατεστημένο. Τρέξε: pip install ipywidgets\")\n",
    "else:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import FloatSlider\n",
    "    from IPython.display import display, clear_output\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Χρώματα για τις κλάσεις\n",
    "    if \"CLASS_COLORS\" in globals():\n",
    "        colors = CLASS_COLORS\n",
    "    else:\n",
    "        colors = [\"C0\", \"C1\", \"C2\"]\n",
    "\n",
    "    # sliders σε raw cm\n",
    "    pl_min, pl_max = float(X[:, 2].min()), float(X[:, 2].max())\n",
    "    pw_min, pw_max = float(X[:, 3].min()), float(X[:, 3].max())\n",
    "\n",
    "    sl_pl = FloatSlider(\n",
    "        value=float(X[:, 2].mean()),\n",
    "        min=pl_min,\n",
    "        max=pl_max,\n",
    "        step=0.05,\n",
    "        description=\"petal length\",\n",
    "        continuous_update=False,\n",
    "    )\n",
    "    sl_pw = FloatSlider(\n",
    "        value=float(X[:, 3].mean()),\n",
    "        min=pw_min,\n",
    "        max=pw_max,\n",
    "        step=0.05,\n",
    "        description=\"petal width\",\n",
    "        continuous_update=False,\n",
    "    )\n",
    "\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def update(petal_length, petal_width):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # δείγμα: sepal στα μέσα τους, petal από sliders\n",
    "            sepal_mean_0 = X[:, 0].mean()\n",
    "            sepal_mean_1 = X[:, 1].mean()\n",
    "            sample_raw = np.array(\n",
    "                [sepal_mean_0, sepal_mean_1, petal_length, petal_width]\n",
    "            ).reshape(1, -1)\n",
    "            sample_scaled = scaler.transform(sample_raw)\n",
    "\n",
    "            proba = gnb.predict_proba(sample_scaled)[0]\n",
    "            pred_idx = int(gnb.predict(sample_scaled)[0])\n",
    "\n",
    "            print(\"Sample (raw):\", np.round(sample_raw.flatten(), 3))\n",
    "            print(\"Posterior probs (GaussianNB):\")\n",
    "            for k, cls in enumerate(class_names):\n",
    "                print(f\"  P({cls} | x) = {proba[k]:.3f}\")\n",
    "            print(\"Predicted class:\", pred_idx, \"→\", class_names[pred_idx])\n",
    "\n",
    "            # plot\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            for k, cls in enumerate(class_names):\n",
    "                mask = (y == k)\n",
    "                plt.scatter(\n",
    "                    X[mask, 2],\n",
    "                    X[mask, 3],\n",
    "                    color=colors[k],\n",
    "                    alpha=0.5,\n",
    "                    label=cls,\n",
    "                )\n",
    "            plt.scatter(\n",
    "                sample_raw[0, 2],\n",
    "                sample_raw[0, 3],\n",
    "                color=\"k\",\n",
    "                marker=\"X\",\n",
    "                s=100,\n",
    "                label=\"sample\",\n",
    "            )\n",
    "            plt.xlabel(\"petal length (cm)\")\n",
    "            plt.ylabel(\"petal width (cm)\")\n",
    "            plt.title(\"Interactive sample overlay on petal length vs width\")\n",
    "            plt.legend(title=\"Class\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    widgets.interactive_output(\n",
    "        update,\n",
    "        {\"petal_length\": sl_pl, \"petal_width\": sl_pw},\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([sl_pl, sl_pw, out]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundaries για GaussianNB πάνω στις petal features (μετά την εκπαίδευση)\n",
    "# Εδώ χρησιμοποιούμε X_train_scaled και y_train που έχουν δημιουργηθεί στο notebook\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Επιλέγουμε τις δύο διαστάσεις που θέλουμε να απεικονίσουμε:\n",
    "# index 2: petal length, index 3: petal width (μετά την τυποποίηση)\n",
    "X2_train = X_train_scaled[:, [2, 3]]\n",
    "X2_val = X_val_scaled[:, [2, 3]]\n",
    "\n",
    "# Εκπαιδεύουμε ένα νέο GaussianNB μόνο με τις δύο αυτές διαστάσεις\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X2_train, y_train)\n",
    "\n",
    "# Δημιουργία meshgrid για προβολή decision boundary:\n",
    "# - x_min..x_max, y_min..y_max προσδιορίζουν την περιοχή που θα καλύψει το mesh\n",
    "x_min, x_max = X2_train[:,0].min() - 0.5, X2_train[:,0].max() + 0.5\n",
    "y_min, y_max = X2_train[:,1].min() - 0.5, X2_train[:,1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Για κάθε σημείο του mesh grid προβλέπουμε την κλάση και μετατρέπουμε το αποτέλεσμα στο σχήμα του grid\n",
    "Z = clf2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Χρώματα για το υπόβαθρο (light). Διατηρούμε ένα απαλό colormap για το background\n",
    "cmap_light = ListedColormap(['#FFEEEE', '#EEFFEE', '#EEEEFF'])\n",
    "# Use discrete class_cmap for points and class_norm for label mapping\n",
    "\n",
    "# Σχεδιάζουμε figure και προβάλουμε το decision boundary με contourf\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
    "\n",
    "# Σχεδιάζουμε τα training σημεία πάνω στο decision boundary για να συγκρίνουμε\n",
    "# Τα σημεία έχουν χρώματα βάσει της πραγματικής κλάσης (y_train)\n",
    "ax.scatter(X2_train[:,0], X2_train[:,1], c=y_train, cmap=class_cmap, norm=class_norm, edgecolor='k', s=40)\n",
    "\n",
    "# Βάζουμε ετικέτες και τίτλο και εμφανίζουμε\n",
    "ax.set_xlabel('petal length (scaled)')\n",
    "ax.set_ylabel('petal width (scaled)')\n",
    "ax.set_title('Decision boundary (GaussianNB) on petal features (train set)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Ερμηνεία του ορίου απόφασης (decision boundary)\n",
    "\n",
    "- Τα χρωματισμένα φόντα δείχνουν τις περιοχές του χώρου χαρακτηριστικών όπου μια κλάση έχει\n",
    "  τη **μέγιστη εκ των υστέρων πιθανότητα (posterior probability)** $P(y=k \\mid x)$.\n",
    "  Εκεί, ο MAP ταξινομητής (argmax πάνω στο posterior) θα προβλέψει αυτή την κλάση.\n",
    "\n",
    "- Τα σημεία πάνω στο διάγραμμα είναι τα δείγματα εκπαίδευσης (training samples) και είναι\n",
    "  χρωματισμένα σύμφωνα με την πραγματική τους κλάση (true class).\n",
    "\n",
    "Θεωρητική σύνδεση με το Gaussian Naive Bayes (GaussianNB):\n",
    "\n",
    "- Το GaussianNB υποθέτει ότι κάθε χαρακτηριστικό (feature), για κάθε κλάση (class),\n",
    "  ακολουθεί μια **Κανονική κατανομή (Gaussian distribution)** με μέσο (mean) και διασπορά (variance)\n",
    "  $(\\mu_{k,i}, \\sigma^2_{k,i})$ και κάνει την υπόθεση υπό όρων ανεξαρτησίας (conditional independence)\n",
    "  μεταξύ των χαρακτηριστικών.\n",
    "\n",
    "- Η εκ των υστέρων πιθανότητα (posterior probability) υπολογίζεται με τον κανόνα του Bayes\n",
    "  $P(y \\mid x) \\propto P(y) \\cdot P(x \\mid y)$, όπου οι **εκ των προτέρων πιθανότητες\n",
    "  (class priors)** $P(y)$ και οι Γκαουσιανές pdfs $P(x \\mid y)$ καθορίζουν το σχήμα των περιοχών απόφασης.\n",
    "\n",
    "- Αν όλες οι κλάσεις είχαν ίδιες διακυμάνσεις (ισοτροπική Gaussian), τα όρια απόφασης θα ήταν\n",
    "  σχεδόν **γραμμικά (linear)**. Με διαφορετικές διακυμάνσεις, εμφανίζονται **καμπύλα / ελλειπτικά**\n",
    "  σύνορα, όπως στο Iris μεταξύ `versicolor` και `virginica`.\n",
    "\n",
    "Τυποποίηση (Standardization):\n",
    "\n",
    "- Οι άξονες του διαγράμματος βρίσκονται σε **τυποποιημένη κλίμακα (standardized z-scores)**, επειδή\n",
    "  εφαρμόσαμε `StandardScaler` πριν από το GaussianNB. Έτσι οι τιμές δεν είναι σε εκατοστά αλλά σε\n",
    "  μονάδες τυπικής απόκλισης (standard deviations από τον μέσο).\n",
    "\n",
    "Συσχέτιση με το Iris dataset:\n",
    "\n",
    "- Η `Iris setosa` έχει μικρές τιμές στα petal χαρακτηριστικά και μικρή διασπορά, άρα το GaussianNB\n",
    "  μαθαίνει μια συμπαγή περιοχή με πολύ υψηλό posterior για setosa και ουσιαστικά **κανένα λάθος**.\n",
    "\n",
    "- Οι `Iris versicolor` και `Iris virginica` έχουν επικαλυπτόμενες κατανομές στα petal features, οπότε\n",
    "  υπάρχει μια ζώνη όπου τα posteriors είναι κοντινά και τα λάθη (misclassifications) είναι αναμενόμενα.\n",
    "\n",
    "**Συμπέρασμα:** Το decision boundary είναι μια οπτική απεικόνιση του πού ο ταξινομητής θεωρεί κάθε κλάση πιο\n",
    "πιθανή, με βάση τους μέσους / διασπορές και τα priors που μάθαμε. Μαζί με το confusion matrix και τις\n",
    "μετρικές (precision, recall, F1) μας δίνει μια πλήρη εικόνα της συμπεριφοράς του μοντέλου.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αποθήκευση του μοντέλου και του scaler (models/*.joblib)\n",
    "# Δημιουργούμε φάκελο models κάτω από bayesian_learning/ (ανάλογα με το cwd του notebook)\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "MODELS_DIR = Path('..') / 'bayesian_learning' / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Αποθηκεύουμε το GaussianNB μοντέλο και τον scaler για μελλοντική χρήση\n",
    "# joblib.dump δημιουργεί ένα αρχείο .joblib με το εκπαιδευμένο αντικείμενο\n",
    "joblib.dump(gnb, MODELS_DIR / 'gaussian_nb_iris.joblib')\n",
    "joblib.dump(scaler, MODELS_DIR / 'gaussian_nb_iris_scaler.joblib')\n",
    "print(f\"Saved model -> {MODELS_DIR / 'gaussian_nb_iris.joblib'}\")\n",
    "print(f\"Saved scaler -> {MODELS_DIR / 'gaussian_nb_iris_scaler.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Παράδειγμα χρήσης του αποθηκευμένου μοντέλου + scaler\n",
    "# Φορτώνουμε τα αρχεία που αποθηκεύσαμε παραπάνω και κάνουμε inference\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "MODELS_DIR = Path('..') / 'bayesian_learning' / 'models'\n",
    "\n",
    "# Φορτώνουμε το joblib αρχείο με το μοντέλο και τον scaler\n",
    "model_loaded = joblib.load(MODELS_DIR / 'gaussian_nb_iris.joblib')\n",
    "scaler_loaded = joblib.load(MODELS_DIR / 'gaussian_nb_iris_scaler.joblib')\n",
    "\n",
    "# Παίρνουμε μερικά δείγματα από το validation set για να συγκρίνουμε τις προβλέψεις\n",
    "X_sample = X_val[:3]\n",
    "X_sample_scaled = scaler_loaded.transform(X_sample)\n",
    "\n",
    "# Προβλέπουμε την κλάση και την πιθανότητα για κάθε δείγμα\n",
    "pred = model_loaded.predict(X_sample_scaled)\n",
    "prob = model_loaded.predict_proba(X_sample_scaled)\n",
    "\n",
    "# Εμφανίζουμε τα αποτελέσματα με κατανόηση: ποιες είναι οι πιθανότητες ανά κλάση\n",
    "for i, (x, p, pr) in enumerate(zip(X_sample, pred, prob)):\n",
    "    print(\"-\"*72)\n",
    "    print(f\"Δείγμα #{i}\")\n",
    "    print(f\"Χαρακτηριστικά: {x}\")\n",
    "    print(f\"Πρόβλεψη κλάσης (index): {p} -> {class_names[p]}\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"  P({cls_name} | x) = {pr[cls_idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix — απεικόνιση για γρήγορη αξιολόγηση\n",
    "fig, ax = plt.subplots()\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_val,\n",
    "    y_pred,\n",
    "    display_labels=class_names,\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    ")\n",
    "ax.set_title(\"Confusion matrix – Gaussian NB (Iris)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Ερμηνεία του confusion matrix\n",
    "\n",
    "- Το confusion matrix είναι ένα 3×3 πλέγμα όπου οι **σειρές** αντιστοιχούν στις πραγματικές κλάσεις\n",
    "  (true labels) και οι **στήλες** στις προβλεπόμενες κλάσεις (predicted labels).\n",
    "- Οι τιμές στη διαγώνιο (diagonal) είναι οι σωστές προβλέψεις (True Positives ανά κλάση).\n",
    "  Οτιδήποτε εκτός διαγωνίου είναι λάθη (misclassifications).\n",
    "\n",
    "Για το συγκεκριμένο παράδειγμα του Iris με GaussianNB:\n",
    "\n",
    "- `setosa`: 10/10 σωστές προβλέψεις (διαγώνιο)  \n",
    "  → TP = 10, FN = 0, FP = 0 → Precision = 1.000, Recall = 1.000.\n",
    "- `versicolor`: 10 σωστές προβλέψεις, 1 δείγμα virginica που ταξινομήθηκε ως versicolor  \n",
    "  → TP = 10, FP = 1, FN = 0 → Precision = 10/(10+1) ≈ 0.909, Recall = 1.000.\n",
    "- `virginica`: 9 σωστές προβλέψεις, 1 virginica που ταξινομήθηκε ως versicolor  \n",
    "  → TP = 9, FN = 1, FP = 0 → Precision = 1.000, Recall = 9/(9+1) = 0.900.\n",
    "\n",
    "Συνολικά:\n",
    "\n",
    "- Accuracy = (10 + 10 + 9) / 30 = 29/30 ≈ 0.967.\n",
    "- Το μοναδικό λάθος αφορά τη σύγχυση ανάμεσα σε `versicolor` και `virginica`, κάτι που είδαμε\n",
    "  και στο decision boundary / scatter plot όπου οι δύο κλάσεις επικαλύπτονται στα petal features.\n",
    "\n",
    "Πρακτικά συμπεράσματα:\n",
    "\n",
    "- Η `setosa` είναι πολύ εύκολα διαχωρίσιμη, με μηδενικά λάθη.\n",
    "- Η επικάλυψη μεταξύ `versicolor` και `virginica` είναι φυσική και εξηγεί τα λίγα λάθη που βλέπουμε.\n",
    "- Το confusion matrix συμπληρώνει τις μετρικές του `classification_report` (precision, recall, F1),\n",
    "  δείχνοντας σε ποια ζεύγη κλάσεων εμφανίζεται η σύγχυση.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior πιθανότητες για μερικά δείγματα του validation set\n",
    "# Εδώ χρησιμοποιούμε την sklearn προκειμένου να δείξουμε τις κανονικοποιημένες posterior πιθανότητες\n",
    "# Σημείωση: gnb.predict_proba(x) επιστρέφει P(y=k | x) για κάθε κλάση\n",
    "\n",
    "# Επιλέγουμε δείκτες εντός του X_val ώστε να δούμε πραγματικά παραδείγματα\n",
    "idx_samples = [0, 5, 10]  # δείκτες μέσα στο X_val\n",
    "X_samples = X_val[idx_samples]\n",
    "X_samples_scaled = X_val_scaled[idx_samples]\n",
    "y_true = y_val[idx_samples]\n",
    "\n",
    "# Προβλέπουμε label και probability για τα δείγματα που επιλέξαμε\n",
    "# gnb.predict_proba υπολογίζει τις normalized posterior πιθανότητες κατά τον ίδιο τρόπο με τον παραπάνω χειρο-υπολογισμό:\n",
    "#   post[k] = (P(y=k) * Π_i P(x_i | y=k)) / Σ_k' (P(y=k') * Π_i P(x_i | y=k'))\n",
    "# Σημείωση: sklearn υπολογίζει αυτόν τον τύπο με log for numerical stability και μετά επιστρέφει κανονικοποιημένες πιθανότητες\n",
    "y_pred_samples = gnb.predict(X_samples_scaled)\n",
    "y_proba_samples = gnb.predict_proba(X_samples_scaled)\n",
    "\n",
    "# Εμφανίζουμε τις πληροφορίες σε αναγνώσιμη μορφή\n",
    "for i, (x, true_label, pred_label, proba) in enumerate(\n",
    "    zip(X_samples, y_true, y_pred_samples, y_proba_samples),\n",
    "):\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"Δείγμα #{i}\")\n",
    "    print(f\"Χαρακτηριστικά (sepal_len, sepal_wid, petal_len, petal_wid): {x}\")\n",
    "    print(f\"Πραγματική κλάση: {class_names[true_label]}\")\n",
    "    print(f\"Πρόβλεψη       : {class_names[pred_label]}\")\n",
    "    print(\"Posterior πιθανότητες (P(y=k | x)):\")\n",
    "    # Εδώ εμφανίζουμε τις normalized posterior (predict_proba) που προκύπτουν από τον κανόνα του Bayes\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"  P({cls_name} | x) = {proba[cls_idx]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
